{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab27195",
   "metadata": {},
   "source": [
    "# Azure AI Evaluation SDK Demo\n",
    "\n",
    "This notebook demonstrates how to use the Azure AI Evaluation SDK to evaluate generative AI applications. The SDK provides tools for assessing the quality, safety, and performance of AI-generated content.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Azure AI Evaluation SDK allows you to:\n",
    "- Evaluate a single response or an entire dataset\n",
    "- Use built-in evaluators for quality, safety, and performance metrics\n",
    "- Create custom evaluators for specific needs\n",
    "- Track evaluation results in Azure AI projects\n",
    "\n",
    "Let's explore how to use this SDK for various evaluation scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791bc298",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install the Azure AI Evaluation SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Azure AI Evaluation SDK\n",
    "!pip install azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44df51",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Let's set up our environment variables and authentication for Azure services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Set up authentication\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Initialize Azure AI project and Azure OpenAI connection\n",
    "# Replace these with your actual values or set as environment variables\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"),\n",
    "}\n",
    "\n",
    "# Model configuration for evaluators that use LLMs\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"deployment_name\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d29de",
   "metadata": {},
   "source": [
    "## Performance and Quality Evaluators\n",
    "\n",
    "The SDK provides several evaluators for assessing the quality of AI-generated content. Let's explore some of these evaluators:\n",
    "\n",
    "### AI-Assisted Quality Evaluators\n",
    "\n",
    "- **GroundednessEvaluator**: Evaluates if the response is grounded in the provided context\n",
    "- **RelevanceEvaluator**: Evaluates if the response is relevant to the query\n",
    "- **CoherenceEvaluator**: Evaluates the logical flow and consistency of the response\n",
    "- **FluencyEvaluator**: Evaluates the grammatical correctness and readability\n",
    "- **SimilarityEvaluator**: Evaluates similarity between two pieces of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AI-Assisted quality evaluators\n",
    "from azure.ai.evaluation import (\n",
    "    GroundednessEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator\n",
    ")\n",
    "\n",
    "# Initialize evaluators\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "coherence_eval = CoherenceEvaluator(model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config)\n",
    "similarity_eval = SimilarityEvaluator(model_config)\n",
    "\n",
    "# Example: Evaluate groundedness of a response based on context\n",
    "context = \"The exact speed of light in a vacuum is 299,792,458 meters per second, a constant used in physics to represent 'c'.\"\n",
    "query = \"What is the speed of light?\"\n",
    "response = \"The speed of light is approximately 299,792,458 meters per second.\"\n",
    "\n",
    "groundedness_score = groundedness_eval(context=context, query=query, response=response)\n",
    "print(\"Groundedness Evaluation:\")\n",
    "print(groundedness_score)\n",
    "\n",
    "# Example: Evaluate relevance of a response to a query\n",
    "relevance_score = relevance_eval(query=query, response=response)\n",
    "print(\"\\nRelevance Evaluation:\")\n",
    "print(relevance_score)\n",
    "\n",
    "# Example: Evaluate coherence of a response\n",
    "coherence_score = coherence_eval(response=response)\n",
    "print(\"\\nCoherence Evaluation:\")\n",
    "print(coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff18d3bb",
   "metadata": {},
   "source": [
    "### NLP-Based Metrics\n",
    "\n",
    "These evaluators use traditional NLP metrics to assess the quality of generated text:\n",
    "\n",
    "- **F1ScoreEvaluator**: Calculates F1 score between reference and generated text\n",
    "- **RougeScoreEvaluator**: Calculates ROUGE metrics\n",
    "- **BleuScoreEvaluator**: Calculates BLEU score\n",
    "- **MeteorScoreEvaluator**: Calculates METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59916fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLP metric evaluators\n",
    "from azure.ai.evaluation import (\n",
    "    F1ScoreEvaluator,\n",
    "    RougeScoreEvaluator,\n",
    "    BleuScoreEvaluator,\n",
    "    MeteorScoreEvaluator\n",
    ")\n",
    "\n",
    "# Initialize evaluators\n",
    "f1_eval = F1ScoreEvaluator()\n",
    "rouge_eval = RougeScoreEvaluator()\n",
    "bleu_eval = BleuScoreEvaluator()\n",
    "meteor_eval = MeteorScoreEvaluator()\n",
    "\n",
    "# Example: Calculate metrics between reference and generated text\n",
    "reference = \"The speed of light in a vacuum is exactly 299,792,458 meters per second.\"\n",
    "generated = \"The speed of light is approximately 299,792,458 meters per second.\"\n",
    "\n",
    "f1_score = f1_eval(reference=reference, response=generated)\n",
    "print(\"F1 Score:\")\n",
    "print(f1_score)\n",
    "\n",
    "rouge_score = rouge_eval(reference=reference, response=generated)\n",
    "print(\"\\nROUGE Score:\")\n",
    "print(rouge_score)\n",
    "\n",
    "bleu_score = bleu_eval(reference=reference, response=generated)\n",
    "print(\"\\nBLEU Score:\")\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed3ae04",
   "metadata": {},
   "source": [
    "## Risk and Safety Evaluators\n",
    "\n",
    "The SDK provides evaluators to detect potentially harmful content:\n",
    "\n",
    "- **ViolenceEvaluator**: Detects violent content\n",
    "- **SexualEvaluator**: Detects sexual content\n",
    "- **SelfHarmEvaluator**: Detects content promoting self-harm\n",
    "- **HateUnfairnessEvaluator**: Detects hate speech or unfair bias\n",
    "- **CodeVulnerabilityEvaluator**: Detects vulnerabilities in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import safety evaluators\n",
    "from azure.ai.evaluation import (\n",
    "    ViolenceEvaluator,\n",
    "    HateUnfairnessEvaluator,\n",
    "    CodeVulnerabilityEvaluator\n",
    ")\n",
    "\n",
    "# Initialize evaluators\n",
    "violence_eval = ViolenceEvaluator(model_config)\n",
    "hate_eval = HateUnfairnessEvaluator(model_config)\n",
    "code_vuln_eval = CodeVulnerabilityEvaluator(model_config)\n",
    "\n",
    "# Example: Check a response for potential issues\n",
    "safe_response = \"To solve this problem, we need to analyze the data carefully and draw conclusions based on evidence.\"\n",
    "code_response = \"Here's a password verification function:\\ndef verify(input_pwd, stored_pwd):\\n    return input_pwd == stored_pwd\"\n",
    "\n",
    "violence_score = violence_eval(response=safe_response)\n",
    "print(\"Violence Evaluation:\")\n",
    "print(violence_score)\n",
    "\n",
    "code_vuln_score = code_vuln_eval(response=code_response)\n",
    "print(\"\\nCode Vulnerability Evaluation:\")\n",
    "print(code_vuln_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5608cd",
   "metadata": {},
   "source": [
    "## Custom Evaluators\n",
    "\n",
    "You can create custom evaluators for specific evaluation needs. Here's an example of a simple code-based evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple custom evaluator to measure answer length\n",
    "class AnswerLengthEvaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # A class is made callable by implementing the special method __call__\n",
    "    def __call__(self, *, response: str, **kwargs):\n",
    "        return {\"value\": len(response)}\n",
    "\n",
    "# Initialize and use the custom evaluator\n",
    "answer_length_eval = AnswerLengthEvaluator()\n",
    "length_result = answer_length_eval(response=\"The speed of light is approximately 299,792,458 meters per second.\")\n",
    "print(\"Answer Length Evaluation:\")\n",
    "print(length_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf89319",
   "metadata": {},
   "source": [
    "## Evaluating Datasets\n",
    "\n",
    "The SDK provides the `evaluate()` function to evaluate an entire dataset with multiple evaluators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluate function\n",
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# Create a sample dataset (in real scenarios, you would load this from a file)\n",
    "import json\n",
    "\n",
    "# Create a sample dataset as a list of dictionaries\n",
    "sample_data = [\n",
    "    {\n",
    "        \"query\": \"What is the speed of light?\",\n",
    "        \"context\": \"The exact speed of light in a vacuum is 299,792,458 meters per second, a constant used in physics to represent 'c'.\",\n",
    "        \"response\": \"The speed of light is approximately 299,792,458 meters per second.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"context\": \"Paris is the capital and most populous city of France. It is located on the Seine River.\",\n",
    "        \"response\": \"Paris is the capital of France.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save the sample data to a JSONL file\n",
    "with open(\"sample_data.jsonl\", \"w\") as f:\n",
    "    for item in sample_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# Evaluate the dataset using multiple evaluators\n",
    "result = evaluate(\n",
    "    data=\"sample_data.jsonl\",  # path to the data file\n",
    "    evaluators={\n",
    "        \"groundedness\": groundedness_eval,\n",
    "        \"answer_length\": answer_length_eval\n",
    "    },\n",
    "    # Column mapping to tell the evaluator which columns to use\n",
    "    evaluator_config={\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"answer_length\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # Optionally provide Azure AI project information to track results\n",
    "    # azure_ai_project=azure_ai_project,\n",
    "    # Output path to save results\n",
    "    output_path=\"./evaluation_results.json\"\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Aggregate Metrics:\")\n",
    "print(result[\"metrics\"])\n",
    "print(\"\\nRow-level Results (first row):\")\n",
    "print(result[\"rows\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b7d66",
   "metadata": {},
   "source": [
    "## Multi-Modal Evaluation\n",
    "\n",
    "The SDK also supports evaluation of multi-modal content, including images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470cf551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This example demonstrates the structure for multi-modal evaluation\n",
    "# Actual execution requires appropriate models and images\n",
    "\n",
    "from azure.ai.evaluation import ViolenceEvaluator\n",
    "import base64\n",
    "\n",
    "# Initialize a safety evaluator for multi-modal content\n",
    "safety_evaluator = ViolenceEvaluator(model_config)\n",
    "\n",
    "# Example of how to structure a conversation with an image\n",
    "# In a real scenario, you would load and encode an actual image\n",
    "sample_conversation = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"What's in this image?\",\n",
    "            \"role\": \"user\",\n",
    "        },\n",
    "        {\n",
    "            # In a real scenario, replace with actual base64 encoded image\n",
    "            \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpg;base64,<ENCODED_IMAGE>\"}}],\n",
    "            \"role\": \"assistant\",\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Example showing how you would evaluate the conversation (commented out as it requires actual images)\n",
    "# safety_score = safety_evaluator(conversation=sample_conversation)\n",
    "# print(\"Safety Score for Multimodal Content:\")\n",
    "# print(safety_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeee065",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Azure AI Evaluation SDK provides a comprehensive toolkit for evaluating generative AI applications. It offers:\n",
    "\n",
    "- **Built-in evaluators** for quality, safety, and performance metrics\n",
    "- **Custom evaluator** capabilities for specific evaluation needs\n",
    "- **Dataset evaluation** for assessing large amounts of data\n",
    "- **Multi-modal evaluation** for content including images\n",
    "- **Integration with Azure AI projects** for tracking evaluation results\n",
    "\n",
    "By using these evaluation tools, you can gain insights into your AI application's performance, identify areas for improvement, and ensure the quality and safety of generated content."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
